---
output:
  html_document:
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

My notes on [Deep Learning Book](http://www.deeplearningbook.org/) Chapter 4 on Numerical Computation.

ML algorithms usually require a high amount of numerical computation, which typically refers to algorithms that solve mathematical problems by methods that update estimates of a solution iteratively, rather than solving analytically. Evaluating a function involving real numbers on a computer can be difficult, because the real numbers must be represented with a finite amount of memory.

## **DL 4.1 overflow and underflow**
The fundamental difficulty in performing continuous math on a digital computer is the need to represent infinitely many real numbers with a finite number of bit patterns. This leads to approximation error, which can just be rounding error. Rounding error is especially problematic when it compounds across many operations, and can cause algorithms to fail.

Forms of rounding error:

- ***underflow***: when numbers near zero are rounded to zero. This is especially a problem when it leads to dividing by zero or taking the log of zero.
- ***overflow***: when numbers with large magnitude are approximated as $\infty$ or $-\infty$.

The softmax function as a function that must be stabilized against underflow and overflow.
$$
\text{softmax}(X)_i = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
$$
Analytically, when all of the $x_i$ are equal to a constant $c$, all of the outputs should be $\frac{1}{n}$. Numerically, this may not occur when $c$ has large magnitude. For very negative $c$, $\exp(c)$ will underflow, causing the denominator to become $0$, and making the output undefined. For very positive $c$, $\exp(c)$ will overflow, again making the output undefined. Both of these difficulties can be resolved by instead evaluating $\text{softmax}(Z)$, where $Z = X- \max_i{x_i}$. The value of the softmax function is not changed analytically by adding or subtracting a scalar from the input vector:
$$
\begin{align}
\text{softmax}(Z)_i & = \frac{\exp(x_i- \max_i{x_i})}{\sum_{j=1}^n \exp(x_j-\max_i{x_i})} \\
                    & = \frac{\frac{\exp(x_i)}{\exp(\max_i{x_i})}}{\sum_{j=1}^n{\frac{\exp(x_j)}{\exp(\max_i x_i)}}} \\
                    & = \frac{\frac{\exp(x_i)}{\exp(\max_i{x_i})}}  {{ \frac{\sum_{j=1}^n \exp(x_j)}{\exp(\max_i x_i)}}} \\
                    & = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)} \\
\text{softmax}(Z)_i & = \text{softmax}(X)_i 
\end{align}
$$

Subtracting $\max_i{x_i}$ makes the largest argument to $\exp$ be $0$, which protects against overflow. Likewise, at least one term in the denominator has a value of $1$, protecting against underflow in the denominator and subsequent division by zero. However, underflow in the numerator can still cause the expression to erroneously evaluate to zero. Then, taking $\log{\text{softmax}(x)}$ would erroneously return $-\infty$. This can be stabilized using the same trick used to stabilize the softmax function above.

## **DL 4.2 poor conditioning**
***Conditioning***: how rapidly a function changes with respect to small changes in its inputs.

Functions that change rapidly in response to small input perturbations can be problematic because rounding error in the inputs can result in large changes in the output.

Consider the function $f(x) = \mathbf{A}^{-1}x$. When $\mathbf{A} \in \mathbb{R}^{n \times n}$ has an eigenvalue decomposition, its ***condition number*** is
$$
\max_{i,j} \left|\frac{\lambda_i}{\lambda_j}\right|
$$
, the ratio of the largest and smallest eigenvalue. When the condition number is large, matrix inversion is particularly sensitive to error in the input.

## **DL 4.3 gradient-based optimization**
***Optimization***: minimizing a function $f(x)$ by altering $x$. (Maximization is simply minimizing $-f(x)$).
The function to be minimized or maximized is called the ***objective function*** or ***criterion***. When the function is to be minimized, it can also be called the ***cost function***, ***loss function***, or ***error function***.

The value that minimizes or maximizes a function can be denoted with a superscript $*$, i.e. $x^* = \arg \min f(x)$.

The derivative of a function is useful for function minimization because it tells us how to change $x$ in order to make a small improvement in $y$. For small enough $\epsilon$, $f(x-\epsilon \text{ sign} (f^\prime(x)))$ is less than $f(x)$. This technique is called ***gradient descent***.

Points where $f^\prime(x) = 0$, are ***critical points*** or ***stationary points***, and can be a ***local minimum***, ***local maximum***, or ***saddle point***. The point with the absolute lowest value of $f(x)$ is a ***global minimum*** (there can be more than one). In deep learning, the functions may have many suboptimal local minima, and many saddle points surrounded by flat regions. This makes optimization difficult, especially for high-dimensional functions. We therefore usually settle for finding a "very low" value of $f$, which is not necessarily a global minimum.

The above can be generalized to functions with multiple inputs $f: \mathbb{R}^n \rightarrow \mathbb{R}$. (In order to minimize the function, the output must still be a single scalar output). In this case, the ***gradient*** $\nabla_xf(x)$ is analogous to the derivative. The ***directional derivative*** in a direction $u$ (a unit vector) is the slope of the function $f$ in direction $u$, i.e. $\frac{\partial}{\partial \alpha} f(x+ \alpha u) = u^T \nabla_xf(x)$. The function $f$ can be minimized by moving in the direction of the negative gradient (This is known as the ***method of steepest descent*** or ***gradient descent***):
$$
x^\prime = x - \epsilon \nabla_x f(x)
$$
, where $\epsilon$ is the ***learning rate***, a positive scalar determining the step size.