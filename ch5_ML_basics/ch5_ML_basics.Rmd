---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Notes on chapter 5 of the [Deep Learning Book](www.deeplearningbook.org) on Machine Learning Basics.

***Machine learning***: a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions.

## **5.1 Learning Algorithms**
> "A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$."

### **5.1.1 The Task, $T$**
Learning is *not* the task.
***Learning*** is the means of attaining the ability to perform the task.

  - E.g., in training a robot to walk, the task is walking.
      - This could be accomplished by writing a program specifying how to walk, or by programming the robot to learn how to walk.

ML tasks are usually described in how the system should process an ***example***- a collection of ***features*** that have been quantitatively measured from some object or event that we want the system to process. An example is typically represented as a vector $\mathbf{x} \in \mathbb{R}^n$, where each entry $x_i$ of the vector is another feature.

Common ML tasks:

  - Classification
  - Classification with missing inputs
  - Regression
  - Transcription: unstructured data representation $\rightarrow$ discrete, textual form
  - Machine translation
  - Structured output: any task where the output is a vector with important relationships between the different elements, e.g. parsing-- mapping a natural language sentence into a tree describing its grammatical structure, and tagging nodes of the trees as being verbs, nouns, adverbs, etc.
  - Anomaly detection
  - Synthesis and sampling: generate new examples similar to the training set
  - Imputation of missing values
  - Denoising: predict a clean example $\mathbf{x} \in \mathbb{R}^n$ from a corrupted example $\tilde{\mathbf{x}} \in \mathbb{R}^n$
  - Density estimation/ probability mass function estimation

### **5.1.2 The Performance Measure, $P$**
To evaluate a ML algorithm, need a quantitative measure of its performance. Usually this performance measure $P$ is specific to the task $T$ being carried out by the system.

For classification, classification with missing inputs, transcription tasks, we often use the ***accuracy*** of the model-- the proportion of examples for which the model produces the correct output, or the ***error rate***-- the proportion of examples for which the model produces an incorrect output. The error rate is the expected 0-1 loss, i.e. it is 0 for a correctly classified example and 1 otherwise.

For other tasks such as density estimation, 0-1 loss doesn't make sense. Instead, we use a performance metric that gives the model a continuous-valued score for each example, most commonly the average log-probability the model assigns to some examples.

Performance measured must be measured on a ***test set*** of data that the model has not been trained in order to tell how well the model generalizes.

### **5.1.3 The Experience, $E$**
ML algorithms-- unsupervised or supervised depending on what kind of experience they are allowed to have during learning.

Most learning algorithms experience an entire ***dataset***-- a collection of many examples, aka ***data points***.

***Unsupervised learning***-- experience a dataset containing many features, and learn useful properties of the structure of the dataset. (Learn $p(\mathbf{x})$ by observing examples of $\mathbf{x}$).

***Supervised learning***-- experience a dataset containing features, where each example has a ***label*** or ***target***. (Learn to predict $\mathbf{y}$ from $\mathbf{x}$, usually by estimating $p(\mathbf{y} \mid \mathbf{x})$ from several examples of $\mathbf{x}$ and $\mathbf{y}$).

Unsupervised learning/supervised learning are not formally defined or completely separate. From the chain rule of probability:
$$
\forall \mathbf{x} \in \mathbb{R}^n, \quad p(\mathbf{x}) = \prod_{i=1}^{n} p(x_i \mid x_1, \dots, x_{i-1})
$$
, meaning that the unsupervised problem of modeling $p(\mathbf{x})$ could be split into $n$ supervised learning problems. Alternatively the supervised learning problem of learning $p(y \mid \mathbf{x})$ could be solved using unsupervised learning to learn the joint distribution $p(\mathbf{x}, y)$ and inferring:
$$
p(y \mid \mathbf{x}) = \frac{p(\mathbf{x},y)}{\sum_{y^\prime}p(\mathbf{x}, y^\prime)}
$$

Some ML algorithms do not just experience a fixed dataset. E.g. ***reinforcement learning*** algorithms interact with an environment, with feedback between the learning system and its experiences (not covered in this book).

Datasets can be commonly described with a ***design matrix*** $\mathbf{X}$, where rows are examples and columns are features. However, this only works for data where each example can be described as a vector of the same size. In supervised learning, the design matrix is paired with a vector of labels $\mathbf{y}$.

## **5.2 Capacity, Overfitting and Underfitting**
What separates machine learning from optimization is that we want the ***generalization/test error*** to be low, not just the training error. 

  - ***Generalization error***
      - the expected value of the error on a new input, where the expectation is across different possible inputs drawn from the distribution of inputs we expect the system to encounter in practice.

To affect test set performance while only observing the training set, need to make some assumptions about how the training and test sets are collected in order to apply statistical learning theory.

The train and test data are generated by a probability distribution over datasets-- the ***data generating process***. We make a set of assumptions known as the ***i.i.d assumptions***: 

  - the examples in each dataset are *independent* from each other
  - train and test set are *identically distributed*
      - i.e. drawn from the same probability distribution as each other
      - The same underlying distribution, known as the ***data generating distribution*** $p_{\text{data}}$, generates every train and test example.

In using a ML algorithm, we sample the training set, use it to choose parameters to reduce training set error, then sample the test set. Under this process, expected test error $\geq$ expected training error. Factors determining how well a ML algorithm will perform are its ability to:

  - make the training error small
  - make the gap between training and test error small
  
***Underfitting***-- when a model is unable to obtain a sufficiently low training error
***Overfitting***-- when the gap between training and test error is too large

Under- vs. overfitting is controlled by a model's ***capacity*** (flexibility/degrees of freedom, etc.). ML algorithms generally perform best when their capacity is appropriate for the true complexity of the task they need to perform and the amount of training data provided.

***Representational capacity***-- determined by the family of functions to choose from when varying parameters
***Effective capacity***-- the actual capacity of the algorithm, limited by the imperfection of optimization

The most well-known means of quantifying model capacity is the ***Vapnik-Chervonenkis dimension***, or VC dimension. The VC dimension measures the capacity of a binary classifier, and is defined as being the largest possible value of $m$ for which there exists a training set of $m$ different $\mathbf{x}$ points that the classifier can label arbitrarily.

## **5.3 Hyperparameters and Validation Sets**

  - ***hyperparameters***
      - parameters which control the behavior of the learning algorithm
      - not adapted by the learning algorithm itself
          - although, possible to design nested learning procedure where one algorithm learns the best hyperparameters for another
      - e.g.:
          - in polynomial regression, the degree of the polynomial is a *capacity* hyperparameter
          - $\lambda$ in weight decay (L2 regularization) 
      - hyperparameters may be chosen and not learned because they are difficult to optimize
          - more often, it is not appropriate to learn the hyperparameter on the training set (also see discussion of bias-variance tradeoff in ESL)
              - this applies to all hyperparameters controlling model capacity (aka flexibility, degrees of freedom)
                  - if learned on the training set, the hyperparameter would always be set to the maximum capacity, resulting in overfitting
          - to address this, need to set hyperparameters based on a ***validation set*** not observed during training of the algorithm
              - the validation set needs to be distinct from the test set, since no aspects of the model, including hyperparameters, should be determined based on the test set
                  - therefore, the validation set is a subset of of the training data
                  
### **5.3.1 Cross-Validation**

  - dividing a dataset into a fixed training and test set can be problematic if it the resulting test set is small
      - small test set implies statistical uncertainty around the estimated average test error
          - makes it difficult to claim that one algorithm performs better than another on a given task
      - alternatively, can repeating training and testing on randomly chosen subsets of the dataset
          - most commonly, ***$k$-fold cross-validation*** is used:
              - split dataset into $k$ disjoint subsets
              - estimate test error by taking the average test error across $i$ trials
                  - on trial $i$, the $i$-th subset is used as the test set and the other $i-1$ subsets are used as the training set
          - no unbiased estimators of the variance of these average error estimators exist, but approximations are typically used

## **5.4 Estimators, Bias, and Variance**

Statistical concepts such as ***parameter estimation***, ***bias***, and ***variance*** are useful to formally characterize the machine learning concepts of ***generalization***, ***underfitting***, and ***overfitting***.

### **5.4.1 Point Estimation**

  - ***point estimation***: the attempt to provide the single "best" prediction of some quantity of interest
      - e.g. a single parameter, a vector of parameters, or even a whole function
      - notation: denote estimates with hats, e.g. $\hat{\theta}$ is an estimate of the true value $\theta$
      - let $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(m)}$ be a set of $m$ i.i.d data points
          - a ***point estimator*** or ***statistic*** is any function of the data:
          $$
          \hat{\theta}_m = g \left(\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(m)} \right)
          $$
            - this definition is very general: $g$ is not required to return a value close to the true value $\theta$, or to even return a value limited to the set of allowed values of $\theta$
            - the frequentist perspective: the true parameter value $\theta$ is fixed but unknown, while $\hat{\theta}$ is a function of the data
                - since the data is drawn from a random process, any function of the data is random
                    - therefore, $\hat{\theta}$ is a random variable
        - point estimation can also refer to the estimation of the relationship between input and target variables, i.e. ***function estimation***.
  - ***function estimation***: trying to predict a variable $\mathbf{y}$ given an input vector $\mathbf{x}$
      - assume there is a function f(\mathbf{x}) describing the approximate relationship between $\mathbf{y}$ and $\mathbf{x}$
          - e.g. $\mathbf{y} = f(\mathbf{x}) + \epsilon$, where $\epsilon$ represents the part of $\mathbf{y}$ not predictable from $\mathbf{x}$
      - we are interested in approximating $f$ with a model/estimate $\hat{f}$
          - $\hat{f}$ is really just a point estimator in function space
              - e.g., linear regression can be interpreted as estimating a parameter $\mathbf{w}$ or estimating a function $\hat{f}$ mapping $\mathbf{x}$ to $y$
                
Now for commonly studied properties of point estimators

### **5.4.2 Bias**

Def ***bias***:
$$
\text{bias} \left( \hat{\theta}_m \right) = \text{E} \left(\hat{\theta}_m \right) - \theta
$$

  - the expectation is over the data (seen as samples from a random variable)
  - an estimator $\hat{\theta}_m$ is:
      - ***unbiased*** if $\text{bias}(\hat{\theta}_m) = 0$
          - i.e., $\text{E} \left(\hat{\theta}_m \right) = \theta$
      - ***asymptotically unbiased*** if $\lim_{m \to \infty} \text{bias} \left(\hat{\theta}_m \right) = 0$
          - i.e., $\lim_{m \to \infty} \text{E} \left(\hat{\theta}_m \right)$
  - unbiased estimators are clearly desirable, but are not always the "best"
      - we may take an increase in bias if it is results in a reduction in variance
      
### **5.4.3 Variance and Standard Error**

How much do we expect an estimator to vary as a function of the data sample?

The ***variance*** of an estimator is simply the variance (thanks for this amazing insight, Deep Learning Book!):
$$
\begin{align}
\text{Var} \left(\hat{\theta} \right) & = \text{E} \left[ \left(\hat{\theta} - \text{E} \left[ \hat{\theta}\right]\right)^2\right] \\
                                      & = \text{E} \left[\hat{\theta}^2 \right] - \text{E}^2 \left[\hat{\theta} \right]
\end{align}
$$
where the random variable is the training set.

Def ***standard error***: $\text{SE} \left(\hat{\theta} \right) = \sqrt{\text{Var}\left(\hat{\theta} \right)}$.
          
Variance or standard error of an estimator provides a measure of how we would expect the estimate computed from data to vary as we independently resample the dataset from the underlying data generating process. It is desirable to have an estimator with low variance.

The ***standard error of the mean***:
$$
\text{SE} \left(\hat{\mu}_m \right) = \sqrt{\text{Var} \left[\frac{1}{m} \sum_{i=1}^m x^{(i)}\right]} = \frac{\sigma}{\sqrt{m}},
$$
where $\sigma^2$ is the true variance of the samples $x^i$. The standard error is often estimated by using an estimate of $\sigma$. Neither the square root of the sample variance nor the square root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation. Both approaches tend to underestimate the true standard deviation, but are still used in practice. The square root of the unbiased estimator of the variance is less of an underestimate. For large $m$, the approximation is quite reasonable.

  - SEM is often useful in ML: the generalization error is often estimated by computing the sample mean of the test set error
      - the accuracy of this estimate is determined by the size of the test set
      - taking advantage of the central limit theorem (i.e., the mean is approxiamtely distributed with a normal distribution), the standard error can be used to compute the probability that the true expectation falls in any chosen interval (think 95% confidence intervals).
          - in ML, it is common to say that an algorithm is better than another if the upper bound of the 95% CI for error of the first algorithm is less than the lower bound of the 95% CI of the other
      
### **5.4.4 Trading off Bias and Variance to Minimize Mean Squared Error** 

  - bias and variance measure two different sources of error in an estimator
      - bias measures the expected deviation from the true value of the function or parameter
      - variance measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause
  - balancing bias and variance when choosing an estimator
      - one way to choose is by cross-validation (empirically, highly successful)
      - alternatively, compare the ***mean squared error*** (MSE) of the estimates:
      $$
      \begin{align}
      \text{MSE}  & = \text{E} \left[\left(\hat{\theta}_m- \theta \right)^2 \right] \\
                  & = \text{E} \left[\left(\hat{\theta} - \text{E} \left[ \hat{\theta}\right] + \text{E} \left[\hat{\theta}\right] - \theta \right)^2 \right] \\
                  & = \text{E} \left[\left(\hat{\theta} - \text{E} \left[ \hat{\theta} \right] \right)^2 + \left(\text{E} \left[\hat{\theta} \right] - \theta \right)^2 + 2 \left(\hat{\theta} - \text{E} \left[ \hat{\theta} \right] \right) \left(\text{E} \left[\hat{\theta} \right] - \theta \right)\right] \\
                  & = \text{E} \left[\left(\hat{\theta} - \text{E} \left[ \hat{\theta} \right] \right)^2 \right] + \text{E} \left[\left(\text{E} \left[\hat{\theta} \right] - \theta \right)^2 \right] +2 \text{E} \left[\left(\hat{\theta} - \text{E} \left[ \hat{\theta} \right] \right) \left(\text{E} \left[\hat{\theta} \right] - \theta \right)  \right] \\
                  & = \text{E} \left[\left(\hat{\theta} - \text{E} \left[ \hat{\theta} \right] \right)^2 \right] + \left(\text{E} \left[\hat{\theta} \right] - \theta \right)^2  +2\left(\text{E}\left[\hat{\theta}\right] - \text{E} \left[ \hat{\theta} \right] \right) \left(\text{E} \left[\hat{\theta} \right] - \theta \right)  \\
                  & = \text{Var}\left(\hat{\theta} \right) + \text{Bias}^2 \left(\hat{\theta} \right)
                  \end{align}
      $$
      - From the ***bias-variance decomposition*** above, it is clear that the MSE incorporates both the bias and the variance
          - estimators with small MSE therefore manage to keep both their bias and variance somewhat in check
      - the ***bias-variance trade-off***: increasing the capacity/flexibility of a model tends to increase variance and decrease bias
      
### **5.4.5 Consistency**

  - we are also concerned with the behaviors of estimators as the amount of training data grows
      - (weak) ***consistency***: as the number of data points $m$ in the dataset grows, the point estimates should converge to the true value of the corresponding parameters. Formally:
      
      $$
      \begin{align}
      \lim_{m \to \infty} \hat{\theta}_m & \overset{p}{\rightarrow} \theta \\
      \\
      & \overset{p}{\rightarrow} \quad \text{indicates convergence in probability, i.e.}\\
      & \hspace{4em} \text{for any } \epsilon > 0, \\
      & \hspace{4em} P \left(\left\lvert \hat{\theta}_m - \theta \right\rvert > \epsilon \right)\to 0 \quad \text{as} \quad m \to \infty
      \end{align}
      $$
      - strong consistency refers to the *almost sure* convergence of $\hat{\theta}$ to $\theta$
          - *almost sure* convergence of a sequence of random variables $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots$ to a value $\mathbf{x}$ occurs when:
          $$
          p\left(\lim_{m \to \infty} \mathbf{x}^{(m)}= \mathbf{x} \right) = 1
          $$
      - consistency ensures that the bias induced by the estimator will diminish as the number of data examples grows
          - the reverse is not true: asymptotic unbiasedness does **not** imply consistency
              - e.g. consider estimating the mean parameter $\mu$ of a normal distribution $\mathcal{N} \left(x; \mu, \sigma^2 \right)$ with a dataset consisting of $m$ samples: $\left\{x^{(1)}, \dots, x^{(m)} \right\}$
                  - $x^{(1)}$, the first example of the dataset *could* be used as an estimator: $\hat{\theta} = x^{(1)}$
                      - $\text{E} \left[\hat{\theta}_m \right] = \theta$, so the estimator is unbiased no matter how many data points are seen
                          - therefore, the estimate is asymptotically unbiased
                          - however, it is **not** a consistent estimator, since it is **not** the case that $\hat{\theta}_m \to \theta$ as $m \to \infty$.

## **5.5 Maximum Likelihood Estimation**
Consider a set of $m$ examples $\mathbb{X} = \left\{\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(m)} \right\}$ drawn independently from the true but unknown data generating distribution $p_{\text{data}}(\mathbf{x})$.

Let $p_{\text{model}}(\mathbf{x}; \mathbf{\theta})$ be a parametric family of probability distributions over the same space as $p_{\text{data}}(\mathbf{x})$, indexed by $\mathbf{\theta}$. In other words, $p_{\text{model}}(\mathbf{x}; \mathbf{\theta})$ maps any configuration $\mathbf{x}$ to a real number estimating the true probability $p_{\text{data}}(\mathbf{x})$.

The maximum likelihood estimator for $\mathbf{\theta}$ is then defined as:
$$
\begin{align}
\mathbf{\theta}_{ML}  & = \underset{\theta}{\text{argmax }} p_{\text{model}}(\mathbb{X}; \mathbf{\theta}) \\
                      & = \underset{\theta}{\text{argmax }} \prod_{i=1}^m p_{\text{model}}(\mathbf{x}^{(i)}; \mathbf{\theta}) & \text{samples i.i.d.}\\
                      & = \underset{\theta}{\text{argmax }} \sum_{i=1}^m \log p_{\text{model}}(\mathbf{x}^{(i)}; \mathbf{\theta}) & \text{to avoid underflow} \\
                      & = \underset{\theta}{\text{argmax }} \frac{1}{m}\sum_{i=1}^m \log p_{\text{model}}(\mathbf{x}^{(i)}; \mathbf{\theta}) & \text{argmax is unchanged by rescaling} \\
                      & = \underset{\theta}{\text{argmax }} \text{E}_{\mathbf{X}\sim \hat p_{\text{data}}} \log p_{\text{model}}(\mathbf{x} ; \mathbf{\theta})
\end{align}
$$
One way to interpret MLE-- view it as minimizing the dissimilarity between the empirical distribution $\hat p_{\text{data}}$ defined by the training set and the model distribution, measured by the KL divergence.
$$
D_{\text{KL}}(\hat p_{\text{data}} \parallel p_{\text{model}}) = \text{E}_{\mathbf{X}\sim \hat p_{\text{data}}} \left[\log \hat p_{\text{data}}(\mathbf{x}) - \log p_{\text{model}}(\mathbf{x}) \right]
$$
The left term, $\log \hat p_{\text{data}}(\mathbf{x})$, is a function only of the data generating process, and not of the model. This means when we train the model to minimize the KL divergence, we need only minimize
$$
-\text{E}_{\mathbf{X}\sim \hat p_{\text{data}}} \left[\log p_{\text{model}}(\mathbf{x}) \right]
$$
, which is the same as the maximization above.

Minimizing the KL divergence corresponds exactly to minimizing the cross-entropy between the distributions.

We can thus see MLE as an attempt to make the model distribution match the empirical distribution $\hat p_{\text{data}}$, in lieu of matching the true generating distribution $p_{\text{data}}$ which we do not have direct access to.

Maximum likelihood = minimization of the negative log-likelihood (NLL) = minimization of cross-entropy.

### **5.5.1 Conditional Log-Likelihood and Mean Squared Error**

### **5.5.2 Properties of Maximum Likelihood**

## **5.6 Bayesian Statistics**

### **5.6.1 Maximum A Posteriori (MAP) Estimation**

## **5.7 Supervised Learning Algorithms**

### **5.7.1 Probabilistic Supervised Learning**

### **5.7.2 Support Vector Machines**
A discriminative model driven by a linear function $\mathbf{w}^T\mathbf{x}$. Predicts positive class when $\mathbf{w}^T\mathbf{x}$ is positive, negative class when $\mathbf{w}^T\mathbf{x}$ is negative.

The ***kernel trick***-- many ML algorithms can be written exclusively in terms of dot products between examples, e.g.
$$
\mathbf{w}^T\mathbf{x} = \sum_{i=1}^m\alpha_i \mathbf{x}^T \mathbf{x}^{(i)}
$$
, where $\mathbf{x^{(i)}}$ is a training example and $\mathbf{\alpha}$ is a vector of coefficients. Rewriting the learning algorithm this way allows us to replace $\mathbf{x}$ by the output of a given feature function $\phi(\mathbf{x})$ and the dot product with a function $k(\mathbf{x}, \mathbf{x}^{(i)}) = \phi(\mathbf{x}) \cdot \phi(\mathbf{x}^{(i)})$ called a ***kernel***. Analogous to the dot product case, we can make predictions using the function
$$
f(\mathbf{x}) = \sum_i\alpha_i k\left(\mathbf{x}, \mathbf{x}^{(i)} \right)
$$

This function is nonlinear with respect to $\mathbf{x}$, but the relationship between $\phi(\mathbf{x})$ and $f(\mathbf{x})$ is linear. Also, the relationship between $\alpha$ and $f(\mathbf{x})$ is linear. The kernel-based function is exactly equivalent to preprocessing the data by applying $\phi(\mathbf{x})$ to all inputs, then learning a linear model in the new transformed space.

The kernel trick is powerful for two reasons. First, it allows us to learn models that are nonlinear as a function of $\mathbf{x}$ using convex optimization techniques that are guaranteed to converge efficiently. This is possible because we consider $\phi$ fixed and optimize only $\mathbf{\alpha}$, i.e., the optimziation algorithm can view the decision function as being linear in a different space. Second, the kernel function $k$ often admits an implementation that is significantly more computational efficient than naively constructing two $\phi(\mathbf{x})$ vectors and explicitly taking their dot product.

In some cases, $\phi(\mathbf{x})$ can even be infinite dimensional, which would result in an infinite computational cost for the naive, explicit approach. In many cases, $k(\mathbf{x}, \mathbf{x^\prime})$ is a nonlinear, tractable function of $\mathbf{X}$ even when $\phi(\mathbf{x})$ is intractable. For example, construct a feature mapping $\phi(x)$ over the non-negative integers $x$. Suppose that this mapping returns a vector containing $x$ ones followed by infinitely many zeros. The kernel function $k(x, x^{(i)}) = \min(x, x^{(i)})$ is exactly equivalent to the corresponding infinite-dimensional dot product.

### **5.7.3 Other Simple Supervised Learning Algorithms**

## **5.8 Unsupervised Learning Algorithms**

### **5.8.1 Principal Components Analysis**

### **5.8.2 $k$-means Clustering**

## **5.9 Stochastic Gradient Descent**
***Stochastic gradient descent*** (SGD)-- an extension of gradient descent that powers nearly all of deep learning.

The recurring problem: large training sets are necessary for good generalization, but large training sets are also more computationally expensive.

Cost functions often decompose as a sum over training examples of some per-example loss function, E.g. the negative conditional log-likelihood of the training data can be written as
$$
J(\mathbf{\theta}) = \text{E}_{\mathbf{X},y\sim\hat p_{\text{data}}} L(\mathbf{x},y,\mathbf{\theta}) = \frac{1}{m}\sum_{i=1}^m L\left(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{\theta} \right)
$$

where $L$ is the per-example loss $L(\mathbf{x},y,\mathbf{\theta}) = -\log p(y \mid \mathbf{x}; \mathbf{\theta})$.

For these additive cost functions, gradient descent requires computing
$$
\nabla_{\mathbf{\theta}} J(\mathbf{\theta}) = \frac{1}{m} \sum_{i=1}^m \nabla_{\mathbf{\theta}} L\left(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{\theta} \right)
$$

. The computational cost of this operation is $O(m)$. As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively long. The insight of stochastic gradient descent: the gradient is an expectation, which can be estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a ***minibatch*** of examples $\mathbb{B} = \left\{\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(m^\prime)} \right\}$ drawn uniformly from the training set. The minibatch size $m^\prime$ is typically chosen to be a relatively small number of examples, ranging from 1 to a few hundred. Crucially, $m^\prime$ is usually held fixed as the training set size $m$ grows. We may fit a training set with billions of examples using updates computed on only a hundred examples.

The estimate of the gradient is formed as
$$
\mathbf{g} = \frac{1}{m^\prime} \nabla_{\mathbf{\theta}} \sum_{i=1}^{m^\prime} L \left(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{\theta} \right)
$$

using examples from the minibatch $\mathbb{B}$. SGD then follows the estimated gradient downhill:
$$
\mathbf{\theta} \gets \mathbf{\theta} - \epsilon \mathbf{g}
$$

## **5.10 Building an ML algorithm**

Nearly all deep learning algorithms combine specification of a ***dataset***, a ***cost function***, an ***optimization procedure***, and a ***model***.
E.g. linear regression:

  - dataset: $\mathbf{X}$ and $\mathbf{y}$
  - cost function: $J(\mathbf{w}, b) = -\text{E}_{\mathbf{X}, y \sim \hat p_{\text{data}}} \log p_{\text{model}}\left(y \mid \mathbf{x} \right)$
  - model: $p_{\text{model}}(y \mid \mathbf{x}) = \mathcal{N}(y; \mathbf{x}^T\mathbf{w}+b,1)$
  - optimization algorithm: solve for zero gradient with normal equations
  
The cost function typically includes at least one term that causes the learning process to perform statistical estimation. The most common cost function is the negative log-likelihood (minimizing NLL = maximum likelihood estimation).

The cost function may also include additional terms, such as regularization terms like weight decay (aka L2, or ridge regression).

If the model is nonlinear, then the cost function usually cannot be optimized in closed form. This requires an iterative numerical optimization procedure, e.g. gradient descent.

In some cases, the cost function may be a function that we cannot actually evaluate, for computational reasons. In these cases, we can still approximately minimize the function using iterative numerical optimization if we have some way of approximating its gradients.

## **5.11 Challenges Motivating Deep Learning**

### **5.11.1 The Curse of Dimensionality**

### **5.11.2 Local Constancy and Smoothness Regularization**

### **5.11.3 Manifold Learning**


